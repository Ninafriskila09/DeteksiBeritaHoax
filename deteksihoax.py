# -*- coding: utf-8 -*-
"""Hoax.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1leIUrcFQFT7BfdMYcpIWXiYb_5Jzm7fT

# Preparation Data
"""

import pandas as pd

# Membaca data dari file CSV dengan menyertakan parameter delimiter
df = pd.read_excel('/content/dataset.xlsx')

df

data = df.drop(columns=['Unnamed: 0', 'Teks Berita'])
data

data = data.drop_duplicates(subset=['Title'])
data['Title'] = data['Title'].str.replace('\[SALAH\]', '')
data

"""# Deteksi Hoax Menggunakan Naive Bayes

## Preprocessing
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm

# Untuk mempermudah, simpan setiap objek agar dapat digunakan untuk pemodelan maupun deployment. Gunakan library Pickle
import pickle
import re
import seaborn as sn

# %matplotlib inline

!pip -q install sastrawi

data["Label"].value_counts()

"""### Casefolding"""

import re

# Buat fungsi untuk langkah case folding
def casefolding(text):
    # Check if text is a string
    if not isinstance(text, str):
        return ""  # Return an empty string or some other placeholder for non-string inputs
    text = re.sub(r'\n', ' ', text)                      # Menghilangkan Enter
    text = text.lower()                                  # Mengubah huruf menjadi huruf kecil
    text = text.replace("[SALAH]", " ")                  # Hapus [SALAH]
    text = text.replace(":", " ")                        # Hapus :
    text = re.sub(r'https?://\S+|www\.\S+', ' ', text)   # Menghapus URL
    text = re.sub(r'[-+]?[0-9]+', ' ', text)             # Menghapus angka
    text = re.sub(r'[^\w\s*]',' ', text)                 # Menghapus karakter tanda baca
    text = text.strip()                                  # Menghapus whitespace di awal dan di akhir
    return text

data["casefolding"] = data["Title"].apply(lambda x: casefolding(x))
data.head(15)

"""### Normalisasi kata"""



"""normalisasi kata, datanya dapat di download disini
https://raw.githubusercontent.com/ksnugroho/klasifikasi-spam-sms/master/data/key_norm.csv
"""

key_norm = pd.read_csv("/content/key_norm.csv")
def text_normalize(text):
    text = ' '.join([key_norm[key_norm['singkat'] == word]['hasil'].values[0] if (key_norm['singkat'] == word).any() else word for word in text.split()])
    text = str.lower(text)
    return text
data['textnormalize'] = data['casefolding'].apply(text_normalize)
data.head()

"""### Stopwords Removal"""

# download Stopwords bahasa indonesia
import nltk
nltk.download('stopwords')

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords


stopwords_ind = stopwords.words('indonesian')
stopwords_ind

def remove_stop_words(text):
    clean_words = []
    text = text.split()
    for word in text:
        if word not in stopwords_ind:
            clean_words.append(word)
    return " ".join(clean_words)
data['stopwordremoval'] = data['textnormalize'].apply(remove_stop_words)
data.head()

"""### Stemming

stemmming adalah menghilangkan kata depan dan kata belakang contohnya

Menghidupkan -> hidup

Module yang digunakan untuk bahasa indonesia adalah Sastrawi
"""

!pip install Sastrawi

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

factory = StemmerFactory()
stemmer = factory.create_stemmer()

# Buat fungsi untuk langkah stemming bahasa Indonesia
def stemming(text):
    text = stemmer.stem(text)
    return text
data['stemming'] = data['stopwordremoval'].apply(stemming)
data.head()

data["clean_text"] = data['stemming']
data.drop(["casefolding","textnormalize","stopwordremoval","stemming"],axis=1,inplace=True)
data.head()

"""### Hapus clean text yang kosong"""

data = data[data["clean_text"]!=""]
data

from wordcloud import WordCloud
import matplotlib.pyplot as plt

all_text = ' '.join(data['clean_text'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud Berita Hoax', fontsize=20)  # Memberikan label 'Word Cloud Tentang Hoax'
plt.show()

fakta = data[data['Label'] == 1]

# Menggabungkan teks dari data yang sudah difilter
all_text = ' '.join(fakta['clean_text'])

# Membuat WordCloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)

# Menampilkan WordCloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud fakta', fontsize=20)
plt.show()

hoax = data[data['Label'] == 0]

# Menggabungkan teks dari data yang sudah difilter
all_text = ' '.join(hoax['clean_text'])

# Membuat WordCloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)

# Menampilkan WordCloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud hoax', fontsize=20)
plt.show()

data.to_excel('dataset_clean.xlsx')

"""## Splitting the data"""

from sklearn.model_selection import train_test_split

X_raw = data["clean_text"]
y_raw = data["Label"]
X_train, X_test, y_train, y_test = train_test_split(X_raw.values, y_raw.values, test_size=0.4, random_state=42)

"""## TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(ngram_range=(1,2))
vectorizer.fit(X_train)

X_train_TFIDF = vectorizer.transform(X_train).toarray()
X_test_TFIDF = vectorizer.transform(X_test).toarray()
X = vectorizer.transform(data["clean_text"]).toarray()
kolom = vectorizer.get_feature_names_out()
train_tf_idf = pd.DataFrame(X_train_TFIDF, columns=kolom)
test_tf_idf = pd.DataFrame(X_test_TFIDF, columns=kolom)
train_tf_idf.head()

"""## Feature Selection"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

chi2_features = SelectKBest(chi2, k=500)
X_kbest_features = chi2_features.fit_transform(train_tf_idf, y_train)

print('Banyaknya fitur awal:', train_tf_idf.shape[1])
print('banyaknya fitur setelah di seleksi:', X_kbest_features.shape[1])

X_kbest_features

"""## Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

NB = GaussianNB()
NB.fit(X_kbest_features,y_train)

from sklearn.metrics import classification_report,confusion_matrix

X_test_chi2 = chi2_features.transform(X_test_TFIDF)
y_pred = NB.predict(X_test_chi2)
print(classification_report(y_test, y_pred))

columns = ['0', '1']
confm = confusion_matrix(y_test, y_pred)
df_cm = pd.DataFrame(confm, index=columns, columns=columns)

ax = sn.heatmap(df_cm, cmap='Greens', annot=True, fmt=".0f")
ax.set_title('Confusion matrix')
ax.set_xlabel('Deteksi sebenarnya')
ax.set_ylabel('Deteksi prediksi')

# Pindahkan label pada sumbu x (deteksi prediksi) ke kanan
# wighted avg bobot pada setiap kelas
# ax.invert_xaxis()

plt.show()